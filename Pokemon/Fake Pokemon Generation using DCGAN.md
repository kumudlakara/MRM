# Fake Pokemon Generation using DCGAN

# GANS
GANs are Generative Adversarial Networks. They consist of two constituent models namely a generator and a discriminator. 
The generator learns to generate/create fake data using a latent vector from the original dataset. The fake data generated by the generator becomes a negative training example for the discriminator. The discriminator learns to differentiate the between the real and fake data. 
  - Initially the generator generates fake data which is easily identified by the discriminator. As the training process continues the generator gets better at generating fake images which are closer to the real images and hence it becomes more and more difficult for the discriminator to differentiate between the real and fake images.
  - Finally if the generator trains well, it is able to fool the discriminator and the discriminator classifies a fake image as real and its accuracy decreases.
 
   - The output of the generator is directly connect to the discriminator input. The classification by the discriminator allows the generator to update its weights by backpropagation. 
# Project

  - The aim of this project is to generate fake pokemons given a dataset of pokemon images.
  - This project uses two different datasets:large_dataset and small_dataset
  - The large_dataset comprises of 11,000+ images of pokemons
  - The small_dataset consits of 806 images of pokemons.

# Code

```sh 
from __future__ import print_function
#%matplotlib inline
import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML

# Set random seed for reproducibility
manualSeed = 999
#manualSeed = random.randint(1, 10000) # use if you want new results
print("Random Seed: ", manualSeed)
random.seed(manualSeed)
torch.manual_seed(manualSeed)
```

After uploading both the datasets on google drive they are mounted in the notebook
```sh
from google.colab import drive
drive.mount('/content/gdrive')
```

Now the variable are defined
```sh
 #Root directory for dataset
dataroot = os.path.join("/content/gdrive/My Drive/images")

# Number of workers for dataloader
workers = 2

# Batch size during training
batch_size = 128

# Spatial size of training images. All images will be resized to this
#   size using a transformer.
image_size = 64

# Number of channels in the training images. For color images this is 3
nc = 3

# Size of z latent vector (i.e. size of generator input)
nz = 100

# Size of feature maps in generator
ngf = 64

# Size of feature maps in discriminator
ndf = 64

# Number of training epochs
num_epochs = 1000

# Learning rates for optimizers
lrd = 0.0003
lrg=0.0003

# Beta1 hyperparam for Adam optimizers
beta1 = 0.5

# Number of GPUs available. Use 0 for CPU mode.
ngpu = 1
```
Here initally the learning rate for the discriminator(lrd) and that for the generator(lrg) are the same. Alternative approaches include setting different initial learning rates for both. Also the original DCGAN paper reqiures the learning rate for both the generator and the discriminator to remain the same i.e. 0.0002 however here the leanring rate is 0.0003 and this is decreased to different extent conditioned on the number of epochs.

Now the dataset and DataLoader are created. The dataset is basically the images and is loaded usng ImageFolder. Various transformations are applied. Dataloader however combines a dataset and sampler and provides an iterable over the given dataset.
The images were downsampled to 64X64 size for an efficient process and data normalization. 

```sh
# We can use an image folder dataset the way we have it setup.
# Create the dataset
dataset = dset.ImageFolder(root=dataroot,
                           transform=transforms.Compose([
                               transforms.Resize(image_size),
                               transforms.CenterCrop(image_size),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                           ]))
# Create the dataloader
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                         shuffle=True, num_workers=workers)

# Decide which device we want to run on
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

# Plot some training images
real_batch = next(iter(dataloader))
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))
```
ouptut:
```sh
/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  "Palette images with Transparency expressed in bytes should be "
<matplotlib.image.AxesImage at 0x7f40a3e991d0>
![Training Images](https://github.com/kumudlakara/MRM/blob/master/Pokemon/Training_images_large.png)
```

# Custom Weights Initialization 
```sh
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
```

# Generator
It learns to create fake data by getting feedback from the discriminator by backpropagation.
The generator takes a latent vector as input and outputs images matching the format of images in the dataset i.e. 64X64X3. These generated images are fed into the discriminator which classifies them as real or fake. Using which the loss is calculated and the generator is penalized accordingly.

# Model
The generator model used here consists of 5 layers. The first 4 layers consist of trasnpose convolution followed by batch normalization followed by ReLU activation function. The fifth layer consists of tanh() function. The ouput of the generator is dimensionally a 64X64X3 image.
Batch normalization is applied after each transpose convolution operation in order to improve the efficiency and stability of the network.

```sh
# Generator Code

class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(0.2,True),
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(0.2,True),
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(0.2,True),
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(0.2,True),
            # state size. (ngf) x 32 x 32
            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 64 x 64
        )

    def forward(self, input):
        return self.main(input)
```
- Modifications to the above model are discussed later.

# Discriminator
It is simply a classifier that tries to distingiush real data from the data created by the generator. The real samples fed into the discriminator are considered positive examples and the fake one are considered negative examples. During training the discriminator classifies the data and. The discirminator loss penalizes it for any wrong classificatons. It then updates its weights by backpropagation.
The discriminator architecture resembles the generator model but here LeakyRELU is used as the activation function in place of ReLU and sigmoid activation function is used at the end.
- LeakyReLU activation function is used instead of ReLU because it allows the pass of a small gradient signal for negative values whereas ReLU doesnot. As a result it makes the gradients from the discriminator flow stronger into the generator. Hence instead of passing 0 as gradient in the backproagation pass it uses a small negative gradient.

# Model
The disciminator consists of 5 layers. The first 4 are Convolution layers followed by batch normalization and the activation function which is LeakyReLU. The final layer uses a sigmoid function.

```sh
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.BatchNorm(ndf),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 16 x 16
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input)
```
# 
Now the genrator and discriminator are created .

- Generator
```sh
# Create the generator
netG = Generator(ngpu).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.2.
netG.apply(weights_init)

# Print the model
print(netG)

!!OUTPUT!!!
```
- Discriminator
```sh
# Create the Discriminator
netD = Discriminator(ngpu).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netD = nn.DataParallel(netD, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.2.
netD.apply(weights_init)

# Print the model
print(netD)

!!!OUTPUT!!!!
```
# Setting Up Loss Functions and Optimizers

The loss fucntion used is BCE Loss which stand for Binary Cross Entropy Loss. It is suited for outputs between 0 and 1 (considering 0 as fake and 1 as real). Both the optimizers i.e. for the generator and discriminator used are Adam Optimizers with beta hyperparameter set as 0.5 and the second hyperpaarmeter as 0.999.
```sh
# Initialize BCELoss function
criterion = nn.BCELoss()

# Create batch of latent vectors that we will use to visualize
#  the progression of the generator
fixed_noise = torch.randn(64, nz, 1, 1, device=device)

# Establish convention for real and fake labels during training
real_label = 1
fake_label = 0

# Setup Adam optimizers for both G and D
optimizerD = optim.Adam(netD.parameters(), lr=lrd, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lrg, betas=(beta1, 0.999))
```

# Training Process

- Discirminator
The discirminator is first fed the real batch and accordingly the real loss is found.It is then back propagated. Then it is fed with the fake images from the geenrator and the fake loss is found again using the loss function. Again the error is back propagated. Then finally the discriminator error is calculated as sum of the fake images error and real images error. After this the discriminator takes a step using the Adam Optimizer.

- Generator
For finding the generator loss we get the output from the discriminator by giving it input as the fake images generated by the generator. Then this output is compared with the real labels and the generator loss is calculated which is then backpropagated. After this the generator is allowed to take a step. 

- Learning Rate 
In the original model the learning rate is not updated however here the learning rates for the generator and discriminator are progressively decreased. This is done to increase the speed of training and also initially the model learns general features and then begins to learn more details and hence better performance. This model although is computationally less expensive it is still less accurate as compared to the original model as it uses a higher initial learning rate and this learning rate remains constant throughout the entire training process hence the learning of features takes place at a constant pace.

- After every 100 epocks the output is also shown in order to keep a visual track of the progress.

```sh
# Training Loop

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0


print("Starting Training Loop...")
# For each epoch
for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):

        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        #print(b_size)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate batch of latent vectors
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)
        # Classify all fake batch with D
        output = netD(fake.detach()).view(-1)
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = netD(fake).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                  % (epoch, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))


        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())
        
        # Check how the generator is doing by saving G's output on fixed_noise
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

        iters += 1
    if (epoch%10 == 0) :
      lrd=lrd-0.2*lrd
    if epoch% 10 == 0 or epoch%11 == 0 or epoch%12 == 0:
      lrg=lrg-0.1*lrg

    if epoch%100 == 0:
      plt.figure(figsize=(20,20))
      plt.subplot(1,2,2)
      plt.axis("off")
      plt.title("Fake Images")
      plt.imshow(np.transpose(img_list[-1],(1,2,0)))
      plt.show()
```

# Results
The results are plotted by means of plotting the generator loss and the discriminator loss. The required optimal condition would be to have the discriminator loss around 0.5 and the generator loss somewhere between 0.5 to 2-3. Having a discriminator loss of 0.5 would mean that there is a 50-50 chance for the discriminator to classify an image as fake or real. This would inturn mean that the generator is efficient enough to produce fake images which are quite similar to the real images and hence difficult to distinguish by the discriminator.
The various hyperparameters can therefore be tuned to achieve this optimal situation.

- The number of epochs can be increased to a certain point to optimize the results as it would allow the model to learn better features however after a certain point the model may begin to overfit the dataset and hence the model performance begins to deteriorate. The model may also experience mode collapse wherein the generator begins to output fake images with very less diversity. Hence, the discirminator ideally should be able to identify generator mode collapse while its happening and assign the collapse point a low probability to force the generator to spread out.


# Other Altenatives

- The generator model can be changed to include Upsampling and followed by convolution to produce an effect similar to ConvTranspose2d function however it doesnot seem to improve the performance of the model and also poses problems involving the dimensions of the target and input labels. 
  - Code:
```sh
#Generator Code

class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            nn.BatchNorm2d(nz),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(nz,ngf*8,4,1,0,bias=False),
            nn.BatchNorm2d(ngf*8),
            nn.ReLU(True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(ngf*8,ngf*4,4,1,0,bias=False),
            nn.BatchNorm2d(ngf*4),
            nn.ReLU(True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(ngf*4,ngf*2,4,2,1,bias=False),
            nn.BatchNorm2d(ngf*2),
            nn.ReLU(True),

        )

    def forward(self, input):
        return self.main(input)
```
- Different activation fucntions such as SELU and LeakyReLU were also tried in the generator model.The results for different combinations are as follows:
- - Generator:SELU and Discriminator:SELU- This produced a very ineffcient model with both generator loss very high around 27 and discriminator loss very low (0.001).
- - Generator:SELU and Discriminator:LeakyReLU- This gave the generator loss as 4.4217 and the discriminator loss as 0.2070 hence performing better than the previous model.
- - ORIGINAL:generator:ReLU and discriminator:LeakyReLU- This gave the generator loss as 2.7402 and discriminator loss as 0.6726 hence proving this alternative to be the best among the others.







[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job. There is no need to format nicely because it shouldn't be seen. Thanks SO - http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax)


   [dill]: <https://github.com/joemccann/dillinger>
   [git-repo-url]: <https://github.com/joemccann/dillinger.git>
   [john gruber]: <http://daringfireball.net>
   [df1]: <http://daringfireball.net/projects/markdown/>
   [markdown-it]: <https://github.com/markdown-it/markdown-it>
   [Ace Editor]: <http://ace.ajax.org>
   [node.js]: <http://nodejs.org>
   [Twitter Bootstrap]: <http://twitter.github.com/bootstrap/>
   [jQuery]: <http://jquery.com>
   [@tjholowaychuk]: <http://twitter.com/tjholowaychuk>
   [express]: <http://expressjs.com>
   [AngularJS]: <http://angularjs.org>
   [Gulp]: <http://gulpjs.com>

   [PlDb]: <https://github.com/joemccann/dillinger/tree/master/plugins/dropbox/README.md>
   [PlGh]: <https://github.com/joemccann/dillinger/tree/master/plugins/github/README.md>
   [PlGd]: <https://github.com/joemccann/dillinger/tree/master/plugins/googledrive/README.md>
   [PlOd]: <https://github.com/joemccann/dillinger/tree/master/plugins/onedrive/README.md>
   [PlMe]: <https://github.com/joemccann/dillinger/tree/master/plugins/medium/README.md>
   [PlGa]: <https://github.com/RahulHP/dillinger/blob/master/plugins/googleanalytics/README.md>
